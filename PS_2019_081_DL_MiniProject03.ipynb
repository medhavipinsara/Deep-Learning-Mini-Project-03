{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO0VqEIifWddyQvkxsPywkU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**COSC 44323- Emerging Technologies in Information Technology**\n","\n","**Mini Project 03 - English to Sinhala Translation System using Transformer Neural Networks**\n","\n","**Student Number: PS/2019/081**\n","\n","**Student Name: K. G. M. Pinsara**\n","\n","\n","---\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"_kiFbu2YCLHX"}},{"cell_type":"markdown","source":["#Importing Relevant Libraries"],"metadata":{"id":"Id2OtRd8D0mM"}},{"cell_type":"markdown","source":["#Preparing Data"],"metadata":{"id":"2MCnUnbdyrjd"}},{"cell_type":"code","source":["import random\n","import tensorflow as tf\n","import string\n","import re\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"ZCX9LPNMEAVZ","executionInfo":{"status":"ok","timestamp":1713735325800,"user_tz":-330,"elapsed":6672,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##Mounting the Google Drive"],"metadata":{"id":"Dsk5UXYkEhNK"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Htq7RUVDEpJl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713735352528,"user_tz":-330,"elapsed":23952,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"faa7fb13-6943-4278-baed-444b6b2e244c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##Reading the Data File"],"metadata":{"id":"FMxmuWMVIEBE"}},{"cell_type":"code","source":["text_file = \"/content/drive/My Drive/Colab_Data_Files/EngSin.txt\"\n","with open(text_file) as f:  # Opening the file in read mode\n","    lines = f.read().split(\"\\n\")[:-1]   # Reading the lines from the file and splitting by newline character"],"metadata":{"id":"5bBeA8hlIJJW","executionInfo":{"status":"ok","timestamp":1713735357318,"user_tz":-330,"elapsed":984,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Print the first 20 lines of the file\n","i = 0\n","for line in lines:\n","  print(line)\n","  i = i + 1\n","  if(i==20):\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fe5mXUuF4zUR","executionInfo":{"status":"ok","timestamp":1713735360050,"user_tz":-330,"elapsed":540,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"78422379-fd6f-43a8-a6d3-4e0eb3004bb8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["you will receive a package in the mail\tඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\n","you will receive a confirmation code after completing the registration\tලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කිරීමේ කේතයක්  හම්බවේවි\n","you will receive a discount on your next purchase\tඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි\n","you will receive a phone call from our customer service team\tඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතුමක් හම්බවේවි\n","you will receive a notification when your order is ready for pickup\tඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීමක් හම්බවේවි\n","you will receive a response to your inquiry within 24 hours\tඔයාට පැය 24ක් ඇතුලත ඔයාගෙ විමසීමට පිළිතුරක්  හම්බවේවි\n","you will receive a gift for your loyalty\tඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n","you will receive an invitation to the event\tඔයාට උත්සවයට ආරාධනාවක් හම්බවේවි\n","you will receive a refund for the returned item\tඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\n","you'll receive an email with the details\tඔයාට විස්තර එක්ක  ඊමේල් එකක් හම්බවේවි\n","you'll receive a package in the mail\tඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\n","you'll receive a refund for the returned item\tඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\n","you'll receive a gift for your loyalty\tඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\n","your mother will receive a car\tඔයාගෙ අම්මට කාර් එකක් හම්බවේවි\n","your brother will receive a car\tඔයාගෙ සහෝදරයට කාර් එකක් හම්බවේවි\n","it's pretty cool\tඒක මාර ලස්සනයි\n","we're inviting you to do that\tඅපි ඔයාට ආරාධනා කරනවා ඒක කරන්න කියලා\n","join us in improving machine translation\tයන්ත්‍ර පරිවර්තනය දියුණු කරන්න අපිත් එක්ක එකතු වෙන්න\n","your translations will be used to train an nmt model\tඔයාගෙ පරිවර්තන NMT මොඩලය පුහුණු කරන්න පාවිච්චි කරාවි\n","we're inviting you to submit translation samples for the zoomnmt training\tඅපි ඔයාට ආරාධනා කරනවා  ZoomNMT පුහුණු කරන්න පරිවර්තන සාම්පල ඉදිරිපත් කරන්න කියලා\n"]}]},{"cell_type":"code","source":["# Print the last 10 lines of the file\n","for x in range(len(lines)-10,len(lines)):\n","  print(lines[x])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bRAXkb038IK0","executionInfo":{"status":"ok","timestamp":1713735364792,"user_tz":-330,"elapsed":405,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"c5f56879-0a22-4e05-8b76-1239c3fb4372"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Once upon a time in a small village there lived a young girl named Lily who had a special gift for growing the most beautiful flowers\tඑක් කලෙක කුඩා ගමක ලිලී නම් තරුණියක් ජීවත් වූ අතර ඇයට ලස්සනම මල් වගා කිරීම සඳහා විශේෂ තෑග්ගක් තිබුණි\n","In a distant land a brave knight named Sir Arthur embarked on a perilous journey to rescue a kidnapped princess from an evil sorcerer\tදුර රටක ශ්‍රීමත් ආතර් නම් නිර්භීත නයිට්වරයා නපුරු මායාකාරයෙකුගෙන් පැහැරගෙන ගිය කුමරියක් බේරා ගැනීමට භයානක ගමනක් ආරම්භ කළේය\n","Emily a curious explorer set out on an adventure to uncover the hidden treasures of an ancient lost city deep in the Amazon rainforest\tකුතුහලය දනවන ගවේෂකයෙකු වන එමිලි ඇමසන් වනාන්තරයේ ගැඹුරින් ගිලිහී ගිය පැරණි නගරයක සැඟවුණු වස්තු සොයා ගැනීමට ත්‍රාසජනක ගමනක් ආරම්භ කළාය\n","In the peaceful town of Willowbrook a mischievous cat named Oliver had a talent for getting into amusing and unexpected predicaments\tසාමකාමී නගරයක් වන විලෝබෲක්හි ඔලිවර් නම් දඟකාර බළලාට විනෝදජනක සහ අනපේක්ෂිත දුෂ්කරතාවන්ට පත්වීමේ දක්ෂතාවයක් තිබුණි\n","Sarah a talented pianist dreamt of performing on the grand stage of Carnegie Hall and spent countless hours practicing her musical craft\tදක්ෂ පියානෝ වාදකයෙකු වන සාරා Carnegie Hall හි මහා වේදිකාවේ රඟ දැක්වීමට සිහින මැවූ අතර ඇගේ සංගීත ශිල්පය පුහුණු කිරීමට පැය ගණන් ගත කළාය\n","Deep in the enchanted forest a group of woodland creatures led by a wise old owl embarked on a quest to save their home from destruction\tවශීකෘත වනාන්තරයේ ගැඹුරින් නුවණැති මහලු බකමූණෙකු විසින් මෙහෙයවන ලද වනාන්තර ජීවීන් කණ්ඩායමක් තම නිවස විනාශයෙන් බේරා ගැනීමේ ගවේෂණයක යෙදී සිටියහ\n","On a sunny summer day a group of friends gathered at the beach for a fun-filled day of sandcastle building swimming and laughter\tඅව්ව සහිත ගිම්හාන දිනයක මිතුරන් පිරිසක් වැලි මාලිගා ගොඩ නැගීම පිහිනීම සහ සිනහවෙන් විනෝදයෙන් පිරුණු දවසක් සඳහා වෙරළට රැස් වූහ\n","In a quaint seaside village a mysterious stranger arrived bringing with them an air of excitement and an unexpected twist to the townspeople's lives\tවිචිත්‍රවත් මුහුදු වෙරළේ ගම්මානයකට අද්භූත ආගන්තුකයෙකු පැමිණියේ ඔවුන් සමඟ උද්දීපනයක් සහ නගර වැසියන්ගේ ජීවිතයට අනපේක්ෂිත පෙරළියක් ගෙන එයි\n","Thomas an aspiring writer found inspiration in the bustling streets of a vibrant city where every corner held a story waiting to be told\tඅභිලාෂකාමී ලේඛකයෙකු වන තෝමස් සෑම අස්සක් මුල්ලක් නෑරම කීමට බලා සිටින කතන්දර ඇති උද්යෝගිමත් නගරයක කාර්යබහුල වීදිවල ආශ්වාදයක් ලබා ගත්තේය\n","In a land of mythical creatures a young dragon named Ember struggled to control her fiery breath while learning valuable lessons of friendship and courage\tමිථ්‍යා ජීවීන් සිටින රටක එම්බර් නම් තරුණ මකරෙක් මිත්‍රත්වයේ සහ ධෛර්‍යයේ වටිනා පාඩම් ඉගෙන ගනිමින් ඇගේ ගිනි හුස්ම පාලනය කිරීමට මහත් පරිශ්‍රමයක් දැරීය\n"]}]},{"cell_type":"markdown","source":["##Spliting the English and Sinhala Translation Pairs"],"metadata":{"id":"sETT7okV9H0I"}},{"cell_type":"code","source":["text_pairs = []\n","for line in lines:\n","    english, sinhala = line.split(\"\\t\") # Split the line by tab to separate English and Sinhala translations\n","    sinhala = \"[start] \" + sinhala + \" [end]\" # Add start and end tokens to the Sinhala translation\n","    text_pairs.append((english, sinhala))  # Append the English-Sinhalah pair to text_pairs list\n"],"metadata":{"id":"dEJRl20G9SWy","executionInfo":{"status":"ok","timestamp":1713735368994,"user_tz":-330,"elapsed":372,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Print 3 randomly chosen pairs\n","for i in range(3):\n","    print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpVxOH2g98La","executionInfo":{"status":"ok","timestamp":1713735371484,"user_tz":-330,"elapsed":2,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"ef5dc28a-86ef-4746-9c1a-7c75adf5f240"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["('where are you going', '[start] කොහෙද උඹ යන්නේ    [end]')\n","(\"i'm very sorry\", '[start] ම\\u200cට සමාවෙන්න [end]')\n","('this is real food', '[start] මේ ඇත්ත කෑම [end]')\n"]}]},{"cell_type":"markdown","source":["## Randomizing the Data"],"metadata":{"id":"4oEu4Xms-JVa"}},{"cell_type":"code","source":["import random\n","random.shuffle(text_pairs)"],"metadata":{"id":"vnr1eNSX-OBl","executionInfo":{"status":"ok","timestamp":1713735375159,"user_tz":-330,"elapsed":3,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Spliting the Data into Training, Validation and Testing"],"metadata":{"id":"iHr0yjDN-avr"}},{"cell_type":"code","source":["num_val_samples = int(0.15 * len(text_pairs))  # Calculate the number of validation samples\n","num_train_samples = len(text_pairs) - 2 * num_val_samples  # Calculate the number of training samples\n","train_pairs = text_pairs[:num_train_samples]  # Assign the first part of shuffled pairs to training set\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]  # Assign the next part to validation set\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]  # Assign the rest to testing set"],"metadata":{"id":"8ZBrAa2Q-gcv","executionInfo":{"status":"ok","timestamp":1713735377857,"user_tz":-330,"elapsed":407,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Print sizes of each set\n","print(\"Total sentences:\",len(text_pairs))\n","print(\"Training set size:\",len(train_pairs))\n","print(\"Validation set size:\",len(val_pairs))\n","print(\"Testing set size:\",len(test_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNMixvPX--P9","executionInfo":{"status":"ok","timestamp":1713735381370,"user_tz":-330,"elapsed":2,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"81e81517-9deb-4cc4-c969-31fdb6c3b1b4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sentences: 80684\n","Training set size: 56480\n","Validation set size: 12102\n","Testing set size: 12102\n"]}]},{"cell_type":"code","source":["len(train_pairs)+len(val_pairs)+len(test_pairs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCvUYAKU_by1","executionInfo":{"status":"ok","timestamp":1713735384612,"user_tz":-330,"elapsed":412,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"9ea71aef-963c-4064-f3b4-6414fe4b6ec0"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["80684"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["##Removing Punctuations"],"metadata":{"id":"Neg9yBC__pXr"}},{"cell_type":"code","source":["strip_chars = string.punctuation + \"¿\"  # Define a string containing punctuation marks and \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")  # Remove \"[\" character from strip_chars\n","strip_chars = strip_chars.replace(\"]\", \"\")  # Remove \"]\" character from strip_chars\n","\n","# Print regex pattern for stripping punctuation marks\n","f\"[{re.escape(strip_chars)}]\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"m9t8POwa_tM0","executionInfo":{"status":"ok","timestamp":1713735387661,"user_tz":-330,"elapsed":430,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"6de4f137-3db2-4577-ab8f-170fc8ba87be"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## Vectorizing the English and Sinhala Test Pairs"],"metadata":{"id":"2BE5ZuOsAbaD"}},{"cell_type":"code","source":["def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)  # Convert input string to lowercase\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")  # Remove punctuation marks from the string\n","\n","vocab_size = 15000  # Define the vocabulary size\n","sequence_length = 20  # Define the sequence length\n","\n","# Initialize TextVectorization layers for source (English) and target (Sinhala) texts\n","source_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","\n","# Extract English and Sinhala texts from train_pairs\n","train_english_texts = [pair[0] for pair in train_pairs]\n","train_sinhala_texts = [pair[1] for pair in train_pairs]\n","\n","# Adapt the source vectorization layer to the English texts\n","source_vectorization.adapt(train_english_texts)\n","# Adapt the target vectorization layer to the Sinhala texts.\n","target_vectorization.adapt(train_sinhala_texts)"],"metadata":{"id":"GzXy7bYsAlDh","executionInfo":{"status":"ok","timestamp":1713735400347,"user_tz":-330,"elapsed":9158,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["##Preparing Datasets for the Translation Task"],"metadata":{"id":"gZspmPXpCf37"}},{"cell_type":"code","source":["batch_size = 64  # Define batch size\n","\n","def format_dataset(eng, sin):\n","    eng = source_vectorization(eng)  # Vectorize English texts\n","    sin = target_vectorization(sin)  # Vectorize Sinhala texts\n","    max_length = tf.maximum(tf.shape(eng)[1], tf.shape(sin)[1])  # Get the maximum sequence length\n","    eng = tf.pad(eng, [[0, 0], [0, max_length - tf.shape(eng)[1]]])[:, :max_length]  # Pad or truncate English sequences\n","    sin = tf.pad(sin, [[0, 0], [0, max_length - tf.shape(sin)[1]]])[:, :max_length]  # Pad or truncate Sinhala sequences\n","    return ({\n","        \"english\": eng[:, :-1],\n","        \"sinhala\": sin[:, :-1],\n","    }, sin[:, 1:])  # Return formatted dataset with English and Sinhala inputs, and shifted Sinhala targets\n","\n","def make_dataset(pairs):\n","    eng_texts, sin_texts = zip(*pairs)  # Unzip English-Sinhala pairs\n","    eng_texts = list(eng_texts)  # Convert to list\n","    sin_texts = list(sin_texts)  # Convert to list\n","    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))  # Create dataset from tensors\n","    dataset = dataset.batch(batch_size)  # Batch the dataset\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)  # Map format_dataset function to the dataset\n","    return dataset.shuffle(2048).prefetch(16).cache()  # Shuffle, prefetch, and cache the dataset\n","\n","# Create training and validation datasets\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","\n","# Print shapes of inputs and targets from the first batch of training dataset\n","for inputs, targets in train_ds.take(1):\n","    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n","    print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n","    print(f\"targets.shape: {targets.shape}\")\n","\n","# Print a sample from the training dataset\n","print(list(train_ds.as_numpy_iterator())[50])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bFhQBd1VClK5","executionInfo":{"status":"ok","timestamp":1713739741166,"user_tz":-330,"elapsed":3025,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"f3b62017-efa1-426a-f609-7b17ec1365a4"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs['english'].shape: (64, 20)\n","inputs['sinhala'].shape: (64, 20)\n","targets.shape: (64, 20)\n","({'english': array([[   51,    55,     0, ...,     0,     0,     0],\n","       [    2,    34,    13, ...,     0,     0,     0],\n","       [   17,  3769,    16, ...,     0,     0,     0],\n","       ...,\n","       [11443,     4,  2279, ...,     0,     0,     0],\n","       [  756, 12835,     3, ...,     0,     0,     0],\n","       [   97,    31,   102, ...,     0,     0,     0]]), 'sinhala': array([[    2,   201,  3467, ...,     0,     0,     0],\n","       [    2,     5,     9, ...,     0,     0,     0],\n","       [    2,    50,   639, ...,     0,     0,     0],\n","       ...,\n","       [    2, 11226,  4791, ...,     0,     0,     0],\n","       [    2,   631,  1359, ...,     0,     0,     0],\n","       [    2,   149,  2241, ...,     0,     0,     0]])}, array([[  201,  3467,     3, ...,     0,     0,     0],\n","       [    5,     9,   394, ...,     0,     0,     0],\n","       [   50,   639,  5005, ...,     0,     0,     0],\n","       ...,\n","       [11226,  4791,     3, ...,     0,     0,     0],\n","       [  631,  1359,     4, ...,     0,     0,     0],\n","       [  149,  2241,   352, ...,     0,     0,     0]]))\n"]}]},{"cell_type":"markdown","source":["#Transformer Model"],"metadata":{"id":"ejtSuoKm14Dl"}},{"cell_type":"markdown","source":["##Transformer Encoder Implemented as a Subclassed Layer"],"metadata":{"id":"S5pRMk2sFRjl"}},{"cell_type":"code","source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        # Multi-head self-attention mechanism\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        # Feed-forward neural network layers\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        # Layer normalization for the two sub-layers\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        # Self-attention mechanism\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        # Add and normalize the self-attention output with the input\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        # Feed-forward network processing\n","        proj_output = self.dense_proj(proj_input)\n","        # Add and normalize the output with the input and self-attention output\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n"],"metadata":{"id":"k0mLqiIGFb56","executionInfo":{"status":"ok","timestamp":1713735417461,"user_tz":-330,"elapsed":392,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["##The Transformer Decorder"],"metadata":{"id":"3kZmzDRtGLgB"}},{"cell_type":"code","source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        # Multi-head self-attention mechanism for decoder input\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        # Multi-head attention mechanism for encoder-decoder attention\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        # Feed-forward neural network layers\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        # Layer normalization for the three sub-layers\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        # Create a causal attention mask to prevent attending to future tokens\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        # Create a causal mask for the decoder input\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            # Combine the input mask with the causal mask\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        else:\n","            padding_mask = mask\n","        # Self-attention mechanism for decoder input\n","        attention_output_1 = self.attention_1(\n","            query=inputs,\n","            value=inputs,\n","            key=inputs,\n","            attention_mask=causal_mask)\n","        # Add and normalize the self-attention output with the input\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        # Attention mechanism for encoder-decoder attention\n","        attention_output_2 = self.attention_2(\n","            query=attention_output_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        # Add and normalize the output with the input and attention output\n","        attention_output_2 = self.layernorm_2(\n","            attention_output_1 + attention_output_2)\n","        # Feed-forward network processing\n","        proj_output = self.dense_proj(attention_output_2)\n","        # Add and normalize the output with the attention output and projection output\n","        return self.layernorm_3(attention_output_2 + proj_output)\n"],"metadata":{"id":"C_UpLwg9GPBz","executionInfo":{"status":"ok","timestamp":1713735422187,"user_tz":-330,"elapsed":395,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Positional Encoding"],"metadata":{"id":"cy8-LkvxGpcH"}},{"cell_type":"code","source":["# Positional embedding layer for incorporating positional information into token embeddings\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        # Embedding layer for token embeddings\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        # Embedding layer for positional embeddings\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        # Generate token embeddings\n","        embedded_tokens = self.token_embeddings(inputs)\n","        # Generate positional embeddings\n","        embedded_positions = self.position_embeddings(positions)\n","        # Add positional embeddings to token embeddings\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        # Create a mask to indicate valid tokens\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config\n"],"metadata":{"id":"tgaQw5MJGsve","executionInfo":{"status":"ok","timestamp":1713735428657,"user_tz":-330,"elapsed":381,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## End-to-End Transformer"],"metadata":{"id":"UYIF93rNHGEH"}},{"cell_type":"code","source":["# Define parameters for the Transformer model\n","embed_dim = 256  # Dimensionality of the token embeddings\n","dense_dim = 2048  # Dimensionality of the feed-forward layer in the transformer blocks\n","num_heads = 8  # Number of attention heads\n","\n","# Define inputs for the encoder and decoder\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")  # Input sequence for the encoder (English)\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")  # Input sequence for the decoder (Sinhala)\n","\n","# Embedding and encoding for the encoder inputs\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)  # Add positional embedding\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)  # Apply TransformerEncoder to encode the input sequence\n","\n","# Embedding, decoding, and output generation for the decoder inputs\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)  # Add positional embedding\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)  # Apply TransformerDecoder to decode the input sequence\n","x = layers.Dropout(0.5)(x)  # Apply dropout regularization to prevent overfitting\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  # Generate output probabilities using a dense layer with softmax activation\n","\n","# Define the end-to-end Transformer model\n","transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)  # Combine encoder and decoder inputs to create the Transformer model\n","\n","# Print model summary\n","transformer.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWTq9X5LHJ18","executionInfo":{"status":"ok","timestamp":1713735436302,"user_tz":-330,"elapsed":3933,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"f980f340-70fe-42c6-d877-229fe5c79062"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," english (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," sinhala (InputLayer)        [(None, None)]               0         []                            \n","                                                                                                  \n"," positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n"," tionalEmbedding)                                                                                 \n","                                                                                                  \n"," positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n"," sitionalEmbedding)                                                                               \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n"," formerEncoder)                                                                                   \n","                                                                                                  \n"," transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n"," formerDecoder)                                                     ',                            \n","                                                                     'transformer_encoder[0][0]'] \n","                                                                                                  \n"," dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n","                                                                                                  \n"," dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 19960216 (76.14 MB)\n","Trainable params: 19960216 (76.14 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Training the Sequence-to-Sequence Transformer"],"metadata":{"id":"PomxLE0oH68i"}},{"cell_type":"code","source":["# Compile the Transformer model with RMSprop optimizer and sparse categorical crossentropy loss\n","transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","# Train the Transformer model on the training dataset for 30 epochs with validation on the validation dataset\n","transformer.fit(train_ds, epochs=30, validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YjfvCl_tICL4","executionInfo":{"status":"ok","timestamp":1713737486171,"user_tz":-330,"elapsed":2040025,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"ef4aa92c-01bf-45b1-eeb8-cc9f7ebb218c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","883/883 [==============================] - 76s 76ms/step - loss: 4.6927 - accuracy: 0.4016 - val_loss: 3.8734 - val_accuracy: 0.4613\n","Epoch 2/30\n","883/883 [==============================] - 59s 67ms/step - loss: 3.9180 - accuracy: 0.4669 - val_loss: 3.5535 - val_accuracy: 0.4907\n","Epoch 3/30\n","883/883 [==============================] - 60s 68ms/step - loss: 3.6122 - accuracy: 0.4950 - val_loss: 3.3709 - val_accuracy: 0.5121\n","Epoch 4/30\n","883/883 [==============================] - 60s 68ms/step - loss: 3.4253 - accuracy: 0.5157 - val_loss: 3.3506 - val_accuracy: 0.5142\n","Epoch 5/30\n","883/883 [==============================] - 64s 72ms/step - loss: 3.2902 - accuracy: 0.5329 - val_loss: 3.3820 - val_accuracy: 0.5116\n","Epoch 6/30\n","883/883 [==============================] - 64s 72ms/step - loss: 3.1904 - accuracy: 0.5477 - val_loss: 3.3034 - val_accuracy: 0.5239\n","Epoch 7/30\n","883/883 [==============================] - 60s 68ms/step - loss: 3.1094 - accuracy: 0.5595 - val_loss: 3.3625 - val_accuracy: 0.5211\n","Epoch 8/30\n","883/883 [==============================] - 64s 72ms/step - loss: 3.0407 - accuracy: 0.5700 - val_loss: 3.3830 - val_accuracy: 0.5235\n","Epoch 9/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.9829 - accuracy: 0.5800 - val_loss: 3.3988 - val_accuracy: 0.5291\n","Epoch 10/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.9331 - accuracy: 0.5891 - val_loss: 3.4286 - val_accuracy: 0.5230\n","Epoch 11/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.8897 - accuracy: 0.5964 - val_loss: 3.5198 - val_accuracy: 0.5172\n","Epoch 12/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.8434 - accuracy: 0.6043 - val_loss: 3.4624 - val_accuracy: 0.5303\n","Epoch 13/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.8096 - accuracy: 0.6106 - val_loss: 3.4712 - val_accuracy: 0.5332\n","Epoch 14/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.7734 - accuracy: 0.6165 - val_loss: 3.5227 - val_accuracy: 0.5315\n","Epoch 15/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.7437 - accuracy: 0.6225 - val_loss: 3.5431 - val_accuracy: 0.5246\n","Epoch 16/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.7112 - accuracy: 0.6274 - val_loss: 3.6116 - val_accuracy: 0.5162\n","Epoch 17/30\n","883/883 [==============================] - 64s 73ms/step - loss: 2.6852 - accuracy: 0.6319 - val_loss: 3.5365 - val_accuracy: 0.5286\n","Epoch 18/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.6596 - accuracy: 0.6364 - val_loss: 3.6017 - val_accuracy: 0.5310\n","Epoch 19/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.6377 - accuracy: 0.6396 - val_loss: 3.6100 - val_accuracy: 0.5296\n","Epoch 20/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.6173 - accuracy: 0.6437 - val_loss: 3.5843 - val_accuracy: 0.5303\n","Epoch 21/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.5935 - accuracy: 0.6479 - val_loss: 3.6116 - val_accuracy: 0.5312\n","Epoch 22/30\n","883/883 [==============================] - 64s 73ms/step - loss: 2.5737 - accuracy: 0.6514 - val_loss: 3.6355 - val_accuracy: 0.5316\n","Epoch 23/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.5499 - accuracy: 0.6557 - val_loss: 3.6433 - val_accuracy: 0.5319\n","Epoch 24/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.5303 - accuracy: 0.6583 - val_loss: 3.6784 - val_accuracy: 0.5263\n","Epoch 25/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.5085 - accuracy: 0.6622 - val_loss: 3.7221 - val_accuracy: 0.5303\n","Epoch 26/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.4896 - accuracy: 0.6655 - val_loss: 3.7276 - val_accuracy: 0.5284\n","Epoch 27/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.4695 - accuracy: 0.6686 - val_loss: 3.7627 - val_accuracy: 0.5286\n","Epoch 28/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.4555 - accuracy: 0.6709 - val_loss: 3.7281 - val_accuracy: 0.5292\n","Epoch 29/30\n","883/883 [==============================] - 64s 72ms/step - loss: 2.4322 - accuracy: 0.6750 - val_loss: 3.7901 - val_accuracy: 0.5318\n","Epoch 30/30\n","883/883 [==============================] - 60s 68ms/step - loss: 2.4142 - accuracy: 0.6785 - val_loss: 3.8139 - val_accuracy: 0.5298\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7fe3c9efdea0>"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["#Testing the Trained Model"],"metadata":{"id":"quS7lyie1q3j"}},{"cell_type":"markdown","source":["##Testing the Transformer on Test Dataset"],"metadata":{"id":"xNHxElXQ0d_m"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Get the vocabulary and create a lookup dictionary for the target language\n","sin_vocab = target_vectorization.get_vocabulary()\n","sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n","\n","# Define the maximum length for decoding a sentence\n","max_decoded_sentence_length = 20\n","\n","# Function to decode a sequence given an input sentence\n","def decode_sequence(input_sentence):\n","    # Vectorize the input sentence\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","\n","    # Initialize the decoded sentence with a start token\n","    decoded_sentence = \"[start]\"\n","\n","    # Loop over the maximum decoded sentence length\n","    for i in range(max_decoded_sentence_length):\n","        # Vectorize the current decoded sentence (excluding the last token)\n","        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n","\n","        # Get predictions from the transformer model\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        # Sample the token index with the highest probability\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","\n","        # Get the corresponding token from the lookup dictionary\n","        sampled_token = sin_index_lookup[sampled_token_index]\n","\n","        # Append the sampled token to the decoded sentence\n","        decoded_sentence += \" \" + sampled_token\n","\n","        # Check if the end token is reached, and break the loop if so\n","        if sampled_token == \"[end]\":\n","            break\n","\n","    return decoded_sentence\n","\n","# Get a list of English sentences from the test pairs\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","\n","# Loop over a number of iterations to decode random sentences\n","for _ in range(20):\n","    # Choose a random input sentence from the list of English sentences\n","    input_sentence = random.choice(test_eng_texts)\n","\n","    # Print the input sentence\n","    print(\"-\")\n","    print(\"English : \", input_sentence)\n","\n","    # Decode the input sentence and print the decoded sequence\n","    print(\"Sinhala : \", decode_sequence(input_sentence))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUfhdV_ptyMk","executionInfo":{"status":"ok","timestamp":1713738438984,"user_tz":-330,"elapsed":5919,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"71cd283a-c854-425b-9981-fac61e44d2a8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["-\n","English :  i don't just talk to spirits\n","Sinhala :  [start] මට කියන්නේ නෑ එහෙම කතා කරන්න [end]\n","-\n","English :   said to let me go  don't let him get away\n","Sinhala :  [start] [UNK] යන්න දෙන්න කිව්වා එයාව අතහරින්න කියලා [end]\n","-\n","English :  ah\n","Sinhala :  [start] ආහ් [end]\n","-\n","English :  go\n","Sinhala :  [start] යන්න [end]\n","-\n","English :  i can't imagine how they found the person who made this\n","Sinhala :  [start] මට හිතාගන්න බෑ මේක කරපු විදිය දැනගත්තා මේ කවුද කියලා [end]\n","-\n","English :  what  \n","Sinhala :  [start] මොකක් [end]\n","-\n","English :  my turn\n","Sinhala :  [start] මගේ [UNK] [end]\n","-\n","English :  i\n","Sinhala :  [start] මම [end]\n","-\n","English :  damn\n","Sinhala :  [start] මගුල [end]\n","-\n","English :  a long time ago\n","Sinhala :  [start] ගොඩක් කාලයක් [end]\n","-\n","English :   come and come mumbaikars  mom\n","Sinhala :  [start] එන්න [UNK] එන්න අම්මා [end]\n","-\n","English :  when it went near my locker did it trip and fall into my locker  \n","Sinhala :  [start] මගේ [UNK] ඇවිත් මගේ [UNK] [UNK] [UNK] වුනත් [end]\n","-\n","English :   මගේ ආදරය දැන් මගේ නොවනු ඇත \n","Sinhala :  [start] මගේ [UNK] එනවා මගේ අම්මා ආවා [end]\n","-\n","English :  all problems disappear\n","Sinhala :  [start] තියෙන ප්‍රශ්න ප්‍රශ්න [end]\n","-\n","English :  the school disciplinary committee does not approve\n","Sinhala :  [start] [UNK] ඉස්කෝලේ [UNK] [UNK] කරන්නේ නැති [end]\n","-\n","English :  you are that gold\n","Sinhala :  [start] ඔයා ඒ මිනිස්සු හදන්නේ [end]\n","-\n","English :  ah come on get on\n","Sinhala :  [start] ආහ් අයියෝ [end]\n","-\n","English :  how are you did you make these  \n","Sinhala :  [start] ඔයා කොහොමද මේවා කලේ [end]\n","-\n","English :  turn on the key\n","Sinhala :  [start] යතුර [UNK] [end]\n","-\n","English :  heyget out\n","Sinhala :  [start] එලියට [UNK] [end]\n"]}]},{"cell_type":"markdown","source":["## Testing the Transformer on User Inputs"],"metadata":{"id":"vFit8AvC-70C"}},{"cell_type":"code","source":["# Get English input from the user\n","input_sentence = input(\"Enter an English sentence: \")\n","\n","# Translate the input sentence to Sinhala\n","translated_sentence = decode_sequence(input_sentence)\n","\n","# Print the translated sentence\n","print(\"Sinhala translation:\", translated_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2dcRh1T07q5","executionInfo":{"status":"ok","timestamp":1713739498088,"user_tz":-330,"elapsed":14491,"user":{"displayName":"Medhavi Pinsara","userId":"09211201514975281503"}},"outputId":"237d0596-5de0-4623-a229-f96a90f9abeb"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter an English sentence: You are beautiful\n","Sinhala translation: [start] ඔයා ලස්සනයි [end]\n"]}]}]}